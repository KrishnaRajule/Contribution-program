{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing/Cleaning data, Build Datasets, One-hot vectors/Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the text file\n",
    "f_list = open(\"train4.txt\").readlines()\n",
    "\n",
    "# Strip all the newline (\\n) character\n",
    "f_lines = [s.rstrip(\"\\n\") for s in f_list]\n",
    "        \n",
    "lines = list()\n",
    "# Split the lines into list of words\n",
    "for i in f_lines:\n",
    "    lines.append(i.split(\" \"))\n",
    "\n",
    "wds = list()\n",
    "# Separate all the words and append them into a list\n",
    "for i in lines:\n",
    "    for j in i:\n",
    "        wds.append(j)\n",
    "        \n",
    "# Remove all the special characters from each word and store all the unique words\n",
    "valid = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "p_valid = [\".\", \",\", \"!\", \"?\", \":\", \";\"]\n",
    "words_dict = list()\n",
    "seq = list()\n",
    "        \n",
    "for i in p_valid:\n",
    "    words_dict.append(i)\n",
    "for word in wds:\n",
    "    s = \"\"\n",
    "    p_s = \"\"\n",
    "    for char in word:\n",
    "        if char in valid:\n",
    "            s += char\n",
    "        if char in p_valid:\n",
    "            p_s = char\n",
    "    seq.append(s)\n",
    "    if s not in words_dict:\n",
    "        words_dict.append(s)\n",
    "    if p_s != \"\":\n",
    "        seq.append(p_s)\n",
    "                \n",
    "words_dict.append(\" \")\n",
    "words_dict.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "\n",
    "X = list()\n",
    "Y = list()\n",
    "Tx = 32\n",
    "len_seq = len(seq)\n",
    "                \n",
    "# Using sequence length we create training examples\n",
    "for i in range(0, (len_seq-Tx), 2):\n",
    "    i_x = i\n",
    "    i_y = i+Tx\n",
    "    X.append(seq[i_x:i_y])\n",
    "    Y.append(seq[(i_x+1):(i_y+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot vector or Embedding\n",
    "\n",
    "m = len(X)\n",
    "nx = len(words_dict)\n",
    "x = np.zeros((m, Tx, nx))\n",
    "y = np.zeros((m, Tx, nx))\n",
    "idx_word = dict((i,w) for i,w in enumerate(words_dict))\n",
    "word_idx = dict((w,i) for i,w in enumerate(words_dict))\n",
    "        \n",
    "# Creating one hot vectors\n",
    "for i, exp in enumerate(X):\n",
    "    for t, wrd in enumerate(exp):\n",
    "        x[i, t, word_idx[wrd]] = 1\n",
    "        y[i, t, word_idx[Y[i][t]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization, Train for number of Epochs, Store/Save Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ty = Tx\n",
    "h_size = 64\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "batch_sz = 20\n",
    "num_batches = int(m/batch_sz)\n",
    "out_loss = 0\n",
    "\n",
    "x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "\n",
    "h_0 = torch.rand(num_layers*2, batch_sz, h_size).type(torch.FloatTensor)\n",
    "c_0 = torch.rand(num_layers*2, batch_sz, h_size).type(torch.FloatTensor)\n",
    "\n",
    "optimizer = optim.Adam((h_0, c_0))\n",
    "        \n",
    "rnn_lstm = nn.LSTM(input_size=nx, hidden_size=h_size, num_layers=num_layers, bias=False, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "linear = nn.Linear(h_size*2, nx)\n",
    "        \n",
    "softmax = nn.Softmax()\n",
    "        \n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = int(input(\"Enter number of epochs: \"))\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(num_batches):\n",
    "        x_batch = x[(j*batch_sz):((j+1)*batch_sz), :, :]\n",
    "        y_batch = y[(j*batch_sz):((j+1)*batch_sz), :, :]\n",
    "        y_batch_m = torch.mean(y_batch, 1, True)\n",
    "        y_batch_s = torch.squeeze(y_batch_m, 1)\n",
    "                \n",
    "        if torch.cuda.is_available():\n",
    "            print(\"CUDA\")\n",
    "            device = torch.device(\"cuda\")\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch_s = y_batch_s.to(device)\n",
    "            h_0 = h_0.to(device)\n",
    "            c_0 = c_0.to(device)\n",
    "                \n",
    "        out, (h_0, c_0) = rnn_lstm(x_batch, (h_0, c_0))\n",
    "        out_l = linear(out)\n",
    "        y_pred = softmax(out_l)\n",
    "        out_loss = loss(y_pred, y_batch_s.type(torch.LongTensor))\n",
    "        out_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "                \n",
    "    print(\"Epoch:\", (i+1), \"\\t Loss:\", out_loss, \"\\n\")\n",
    "            \n",
    "torch.save(h_0, \"Parameters/h.pt\")\n",
    "torch.save(c_0, \"Parameters/c.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Sampling and Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
